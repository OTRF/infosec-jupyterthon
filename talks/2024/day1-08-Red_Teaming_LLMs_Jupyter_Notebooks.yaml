title: 'Red Teaming LLMs with Jupyter Notebooks: A Practical Guide'
speaker:
  - name: Pete Bryan
    job_title: Principal AI Security Researcher
    company: Microsoft
    twitter: "@PeteABryan"
    github:
    picture: PeteBryan.png
    bio: >-
      As a Principal AI Security Researcher at Microsoft, I have over eight years of experience in researching and developing solutions for various security challenges.
      I currently work on the AI Red Team, a group of experts who proactively test and attack the AI systems that Microsoft is building, to identify and mitigate potential risks and vulnerabilities.
      I also have a strong background in threat intelligence and cloud security, having previously led the Sentinel Threat Research team and worked for the Microsoft Threat Intelligence Center.
      I have created and published multiple queries and notebooks for Microsoft Sentinel, the cloud-native SIEM and SOAR solution, using KQL and Python.
      I develop MSTICPy, a Python library for security analysts and researchers.
date: '2024-02-15'
time: '18:10'
abstract: >-
  Large language models (LLMs) are powerful tools for natural language generation and understanding, but they also pose significant challenges and risks.
  To ensure the safety and reliability of LLMs, it is essential to test them on various security and safety aspects. However, testing LLMs manually can be time-consuming and inefficient.
  In this talk, we present an approach for red teaming LLMs using Jupyter Notebooks.
  We show how Jupyter Notebooks can be used to iterate and vary attack techniques to maximize coverage, as well as connect to LLMs to help in developing attacks.
  We discuss the benefits and limitations of our approach, as well as the best practices and recommendations for red teaming LLMs via Jupyter Notebooks.